---
title: "logisticReg"
format: html
editor: visual
---

## load packages and read data

```{r}
library(tidyverse)
library(caret)
library(ggplot2)
library(GGally)
library(ISLR2)
library(pROC)
library(car)

set.seed(12345)
train <- read.csv("output data/train_AFTER_EDA.csv")
train$Dropout <- factor(ifelse(train$Dropout == 1, "Yes", "No"))
train <- train %>% select(-c(StudentID,cohort))
```

## call Confusion Matrix to get F1

```{r}
call <- function(cm) {
  print(cm$byClass["Sensitivity"])
  print(cm$byClass["Specificity"])
  print(cm$byClass["Precision"])
  print(cm$byClass["Recall"])
  print(cm$byClass["F1"])
  print(cm$overall["Accuracy"])
}
```

## set 10 cross validation and partition

```{r}
trctrl <- trainControl(method = "cv", number = 10, classProbs = T)
intrain <- createDataPartition(train$Dropout, p = 0.75, list = FALSE)
trainModel <- train[intrain, ]
testModel <- train[-intrain, ]
```

## step 1 try logistic regression for all variables

```{r}
logicModel <- train(Dropout ~., 
                    method = "glm", 
                    preProcess = c("scale", "center"), 
                    data = trainModel, 
                    trControl = trctrl,
                    family = binomial(link = "logit"))
summary(logicModel)
logic_pred <- predict(logicModel, newdata = testModel)
cm <- confusionMatrix(logic_pred, testModel$Dropout)
call(cm)
```

## step 2 delete some variables that can nearly linear

Because there are many warnings in the console, so I decided to use some ways to eliminate some of the variables.

```{r}
colnames(trainModel[nearZeroVar(trainModel)])
```

By finding variables that may have var tends to have zero, we want to delete those variables and run again the model. We want to delete `total_Scholarship`, `total_Work_Study`.

```{r}
logicModel <- train(Dropout ~. -total_Scholarship -total_Work_Study , 
                    method = "glm", 
                    preProcess = c("scale", "center"), 
                    data = trainModel, 
                    trControl = trctrl,
                    family = binomial(link = "logit"))
summary(logicModel)
logic_pred <- predict(logicModel, newdata = testModel)
cm <- confusionMatrix(logic_pred, testModel$Dropout)
call(cm)

```

## 结论

We tried logistic regression as it is a classical way to find the probability that separate binary data. During the regression process, we have tried eliminate some of the variables that are nearly linear.

We have also used `vif` to try to eliminate some variables that are variance-inflated, but the result is not good.

We have also tried lasso and ridge to get rid of Multicollinearity, but it also seems to have a less F1.

Performance metrics

Accuracy for testing data: 0.8870757

Precision for testing data: 0.8937917

Recall rate for testing data: 0.9261031

F1 score for testing data: 0.9096606
