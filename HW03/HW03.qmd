---
title: "HW03"
author: "Shijie An"
format: html
editor: visual
---

```{r setup}
library(caret)
library(tidyverse)
library(ISLR2)
library(pROC)
```

1.  Load the Auto.csv file on Canvas into a dataframe. A description of the dataset can be found at [[https://rdrr.io/cran/ISLR/man/Auto.html]{.underline}](https://rdrr.io/cran/ISLR/man/Auto.html)

```{r}
auto <- read.csv("Auto.csv")
```

2.  Create and add a binary variable, mpg_high_low, to the dataset that contains a Yes if mpg is a value above 30, and a No if mpg is a value less than or equal to 30. Make sure the mpg_high_low is of type factor.

```{r}
auto <- auto %>%
  mutate(mpg_high_low = factor(ifelse(mpg > 30, "Yes", "No")))
# auto <- auto %>%
#   mutate(mpg_high_low = factor(x= mpg_high_low, 
#                                level = c(0, 1), 
#                                labels = c("Yes", "No")))
```

3.  Split the dataset into 75% training and 25% test and use 10 fold cross validation for the models below

```{r}
set.seed(3333)
# 10 fold cross validations 
trctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = T,
                     )

# separate data
intrain <- createDataPartition(auto$mpg_high_low, p = 0.75, list = FALSE)
train <- auto[intrain, ]
test <- auto[-intrain, ]

```

4.  Fit a logistic regression model to the training set to predict mpg_high_low using all the other features/variables except mpg, year, origin, and name. Which variables are statistically significant? Predict the mpg_high_low using the test dataset and report the accuracy, specificity, sensitivity, and F measure.
```{r}
## This is a function to print all the accuracy, specificity, and sensitivity from confusionMatrix 
call_ass <- function(cm) {
  print(paste("The accuracy is ", cm$overall["Accuracy"]))
  print(paste("The specificity is ", cm$byClass["Specificity"]))
  print(paste("The sensitivity is ", cm$byClass["Sensitivity"]))
}
```

```{r}
## This is just to remind myself. 
## R不智能啊，还得重新建两个dataset来删掉值
train2 <- select(train, -c(mpg, year, origin, name))
logicModel <- train(mpg_high_low ~ .,
                    method = "glm", 
                    preProcess=c('scale', 'center'),
                    data = train2, 
                    trControl = trctrl, 
                    family = binomial(link = "logit"))
summary(logicModel)
test2 <- select(test, -c(mpg, year, origin, name))

logic_pred <- predict(logicModel, newdata = test2)
summary(logic_pred)
###################################################################
l_cm <- confusionMatrix(logic_pred, test2$mpg_high_low)
call_ass(l_cm)
print(paste("The F measure is ", F_meas(logic_pred, test2$mpg_high_low))) 
print("It shows that horsepower is statistically significant")
```



5.  Check if the auto data is imbalanced with respect to mpg_high_low. Report the percentage of the data that belong to the two classes (Yes and No).

```{r}
pt<- prop.table(table(auto$mpg_high_low))
print(paste("It shows Yes and No is imbalanced because No get a ", 
            round(pt[1] * 100, digits = 2), 
            "% and Yes get a ", round(pt[2] * 100, digits = 2), "%."))
```

6.  Alter the threshold for classifying a No to 0.6 and report the increase in specificity and the corresponding loss in accuracy.

```{r}
class_prob <- predict(logicModel, newdata = test2, type = "prob")
head(class_prob)
pred_with_threshold <- as.factor(ifelse(class_prob$Yes > 0.6, "Yes", "No"))
lt_cm <- confusionMatrix(pred_with_threshold, test2$mpg_high_low, positive = "No")
call_ass(lt_cm)
print(paste("There is a loss of accurarcy of ", 
            l_cm$overall["Accuracy"] - lt_cm$overall["Accuracy"]))
print(paste("There is a loss of specificity of ", 
            l_cm$byClass["Specificity"] - lt_cm$byClass["Specificity"]))
```


7.  Fit a Naïve Bayes model to the training data to predict mpg_high_low using all the other features/variables except mpg, year, origin, and name. Predict the mpg_high_low using the test dataset. Plot the ROC curve and report the best threshold on the ROC curve plot. Report the AUC on the curve plot as well. Report the accuracy, sensitivity and specificity.

```{r}
nb_fit <- train(mpg_high_low ~., method = "nb", 
                data = train2,
                preProcess=c('scale', 'center'), 
                trControl = trctrl)
nb_fit
nb_pred <- predict(nb_fit, newdata = test2)
nb_cm <- confusionMatrix(nb_pred, test2$mpg_high_low)
call_ass(nb_cm)
######################################################## plot
roc_curve <- roc(as.numeric(test2$mpg_high_low), class_prob$No)
roc_curve$auc
print(paste("The auc is ", roc_curve$auc))

plot(roc_curve, print.thres = "best", print.auc = T)
coords(roc_curve, "best", ret="threshold")
print(paste("The best roc_curve is ", coords(roc_curve, "best", ret="threshold")))
```


8.  Fit a KNN model to the training data to predict mpg_high_low using all the other features/variables except mpg, year, origin, and name. Use a grid search between 3 and 10 to find the best value of k. Report the accuracy, sensitivity, specificity, precision and recall.

```{r}
knn_fit <- train(mpg_high_low ~ ., 
                 data = train2, 
                 method = "knn", 
                 tuneGrid = expand.grid(k = 3:10),
                 trControl = trctrl, 
                 preProcess=c('scale', 'center')
                 )
knn_fit
knn_pred <- predict(knn_fit, newdata = test2)
k_cm <- confusionMatrix(knn_pred, test2$mpg_high_low)
call_ass(k_cm)
print(paste("The precision is ", precision(knn_pred, test2$mpg_high_low)))
print(paste("The recall is ", recall(knn_pred, test2$mpg_high_low)))
print("The best k is when k = 4, it has an accuracy of 0.8374")
```


9.  Fit a LDA model to the training data to predict mpg_high_low using all the other features/variables except mpg, year, origin, and name. Report the accuracy sensitivity and specificity.

```{r}
lda_fit <- train(mpg_high_low ~ ., method = "lda",
                 data = train2, 
                 trControl = trctrl)
lda_pred <- predict(lda_fit, newdata = test2)
lda_cm <- confusionMatrix(lda_pred, test2$mpg_high_low)
call_ass(lda_cm)
```


10. Summarize the performance of the all the above models by creating a dataframe with 4 columns -- *Model_Name*, *Accuracy*, *Sensitivity*, *Specificity*. The data frame should contain one row for each model you built above with each of the columns filled in with the appropriate metric. Print out the dataframe. Which model performed the best from an accuracy point of view and which model performed best from a specificity point of view without adjusting for the threshold?

```{r}
# row1 <- c("Logistic Regression", l_cm$overall["Accuracy"], 
#           l_cm$byClass["Sensitivity"], l_cm$byClass["Specificity"])
# row2 <- c("Logistic Regression t = 0.6", lt_cm$overall["Accuracy"], 
#           lt_cm$byClass["Sensitivity"], lt_cm$byClass["Specificity"])
# row3 <- c("Naïve Bayes model", nb_cm$overall["Accuracy"], 
#           nb_cm$byClass["Sensitivity"], nb_cm$byClass["Specificity"])
# row4 <- c("KNN model", k_cm$overall["Accuracy"], 
#           k_cm$byClass["Sensitivity"], k_cm$byClass["Specificity"])
# row5 <- c("Lda model", lda_cm$overall["Accuracy"], 
#           lda_cm$byClass["Sensitivity"], lda_cm$byClass["Specificity"])

acc <- function(cm){
  cm$overall["Accuracy"]
}
sens <- function(cm) {
  cm$byClass["Sensitivity"]
}
spec <- function(cm) {
  cm$byClass["Specificity"]
}

Model_Name = c("Logistirc Regression", "Logistic Regression t=0.6", 
                        "Naïve Bayes model","KNN model","Lda model")
Accurarcy = c(acc(l_cm), acc(lt_cm), acc(nb_cm), acc(k_cm), acc(lda_cm))
Sensitivity = c(sens(l_cm), sens(lt_cm), sens(nb_cm), sens(k_cm), sens(lda_cm))
Specificity = c(spec(l_cm), spec(lt_cm), spec(nb_cm), spec(k_cm), spec(lda_cm))
df <- data.frame(Model_Name, Accurarcy, Sensitivity, Specificity)
df
print("KNN model has the best Accurarcy of 0.8969, and Naive Bayes have the best Specificity of 1.00")

```
