college <- college[, -1]
colnames(college)
library(tidyverse)
library(caret)
#1
video_data <- read_csv("C:/Columbia/Educational Data Mining/Data/video-data.csv")
#1
video_data <- read_csv("video-data.csv")
#1
video_data <- read_csv("video-data.csv")
#1
video_data <- read_csv("/Users/anshijie/Desktop/D/CU TC/HUDK 4050 data mining/week 4 Non Linear Regression/video-data.csv")
#2
video_data <- na.omit(video_data)
colnames(video_data)
#3
#Split data into train and test
intrain <- createDataPartition(video_data$watch.time,p=0.75,list = FALSE)
train1 <- video_data[intrain,]
test1 <- video_data[-intrain,]
intrain <- createDataPartition(college, p = 0.8, list = FALSE)
colnames(college)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
train1
reg_model <- train(Top10perc ~ . -stid, data=train1, method ="lm")
colnames(video_data)
colnames(college)
reg_model <- train(Grad.Rate ~ . , data=train1, method ="lm")
summary(reg_model)
ï¼ŸcreateDataPartition()
?createDataPartition
colnames(college)
colnames(reg_model)
reg_model$finalModel
summary(reg_model)
summary(reg_model$finalModel)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I found Apps and Outstate are significant at p = 0.001 level.
testpredictions <- predict(reg_model, newdata = test1)
summary(testpredictions)
postResample(pred = testpredictions, obs = college$Top10perc)
?postResample
postResample(pred = testpredictions, obs = college$Grad.Rate)
testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
postResample((pred = testprediction1, obs = college$Grad.Rate))
postResample(pred = testprediction1, obs = college$Grad.Rate)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
test1 <- college[-intrain, ]
test1 <- college[-intrain, ]
test1 <- college[-intrain, ]
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
postResample(pred = testprediction1, obs = college$Grad.Rate)
?postResample
college <- read.csv("College.csv")
missing <- which(is.na(college))
missingCol <- floor(missing / nrow(college)) + 1
missingCol
college$PhD[is.na(college$PhD)] <- ceiling(mean(college$PhD, na.rm = TRUE))
college$Grad.Rate[is.na(college$Grad.Rate)] <- ceiling(
mean(college$Grad.Rate, na.rm = TRUE))
college <- college[, -1]
colnames(college)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I found Apps and Outstate are significant at p = 0.001 level.
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = college$Grad.Rate)
postResample(pred = testpredictions, obs = test1$Grad.Rate)
postResample(pred = testprediction1, obs = train1$Grad.Rate)
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
postResample(pred = testprediction1, obs = train1$Grad.Rate)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I found Apps and Outstate are significant at p = 0.001 level.
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = test1$Grad.Rate)
testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
postResample(pred = testprediction1, obs = train1$Grad.Rate)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I found Apps and Outstate are significant at p = 0.001 level.
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = test1$Grad.Rate)
postResample(pred = testpredictions, obs = test1$Grad.Rate)
install.packages("leaps")
library(leaps)
library(caret)
library(tidyverse)
#Set the seed for repeatable results
set.seed(3333)
#Load the dataset
library(ISLR2)
Hitters <- na.omit(Hitters)
# Best subset selection is not available in caret so we will use the leaps library
library(leaps)
# Function regsubsets() in the leaps library performs best subset selection
# set the nvmax paraemter to the number of independent variables
# you are using in the model (default if you omit this parameter is 8)
regfit_full <- regsubsets(Salary ~ .,data=Hitters,nvmax=19)
# The summary() command outputs the best set of variables for each model size.
# The stars in the output tell you which variables are chosen for the corresponding model
reg_summary <- summary(regfit_full)
reg_summary
#You can look at the r squared, adjusted r squared, BIC and Cp for each of the models with below code
reg_summary$rsq
reg_summary$adjr2
reg_summary$bic
reg_summary$cp
#The which.max() or which.min() command lets you see which model is best based on the criteria you choose
which.min(reg.summary$bic)
# The summary() command outputs the best set of variables for each model size.
# The stars in the output tell you which variables are chosen for the corresponding model
reg_summary <- summary(regfit_full)
reg_summary
#You can look at the r squared, adjusted r squared, BIC and Cp for each of the models with below code
reg_summary$rsq
reg_summary$adjr2
reg_summary$bic
reg_summary$cp
#The which.max() or which.min() command lets you see which model is best based on the criteria you choose
which.min(reg.summary$bic)
# Forward and Backward Stepwise Selection can be done in caret and also leaps
# Using leaps
regfit_fwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
reg_summary <- summary(regfit_fwd)
which.min(reg_summary$bic)
regfit_bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
reg_summary <- summary(regfit_bwd)
which.min(reg_summary$bic)
# Using caret
library(caret)
#Use caret to create training and test datasets
#Create training and test datasets. The p parameter determines the split(e.g. 75/25)
intrain <- createDataPartition(Hitters$Salary,p=0.75,list = FALSE)
train1 <- Hitters[intrain,]
test1 <- Hitters[-intrain,]
#Setup the cross validation. method is the type of cross validation and number is the number of folds
trctrl <- trainControl(method = "cv", number = 10)
#Backward subset selection in caret
#Create a grid for searching through the specified number of variables
nvmax <- data.frame( nvmax= seq(1, 19, length = 19))
backward_fit <- train(Salary ~., data = train1, method = "leapBackward",
trControl=trctrl, tuneGrid = nvmax)
print(backward_fit)
summary(backward_fit)
#Forward subset selection in caret
forward_fit <- train(Salary ~., data = train1, method = "leapForward",
trControl=trctrl, tuneGrid = nvmax)
print(forward_fit)
summary(forward_fit)
#Ridge regression using caret
#Train the model on the training dataset
#If you want caret to automatically select the lambda, do not specify lambda parameter
#The preProcess argument does a variety of data pre processing tasks like scaling and centering
ridge_fit <- train(Salary ~., data = train1, method = "ridge",
trControl=trctrl, preProcess=c('scale', 'center'))
# See the alpha and lambda parameters used
elasticnet$bestTune
#Calculate Regression Performance Metrics - Mean Square Error (MSE), RMSE, MAE
postResample(pred = elastic_pred, obs = test1$Salary)
#Calculate Regression Performance Metrics - Mean Square Error (MSE), RMSE, MAE
postResample(pred = elastic_pred, obs = test1$Salary)
#Generate predictions with test dataset
elastic_pred <- predict(elasticnet, test1)
# ElasticNet
elasticnet <- train(Salary ~., data = train1,
method='glmnet',
preProc=c('scale','center'),
trControl=trctrl)
Hitters
?regsubsets
regfit_bwd <- regsubsets(Grad.Rate ~ ., data = train1, nvmax = 18, method = "backward")
colnames(train1)
library(leaps)
library(caret)
library(tidyverse)
college <- read.csv("College.csv")
missing <- which(is.na(college))
missingCol <- floor(missing / nrow(college)) + 1
missingCol
college$PhD[is.na(college$PhD)] <- ceiling(mean(college$PhD, na.rm = TRUE))
college$Grad.Rate[is.na(college$Grad.Rate)] <- ceiling(
mean(college$Grad.Rate, na.rm = TRUE))
college <- college[, -1]
colnames(college)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
postResample(pred = testprediction1, obs = train1$Grad.Rate)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I found Apps and Outstate are significant at p = 0.001 level.
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = test1$Grad.Rate)
regfit_bwd <- regsubsets(Grad.Rate ~ ., data = train1, nvmax = 18, method = "backward")
regfit_bwd <- regsubsets(Grad.Rate ~ ., data = train1, nvmax = 18, method = "backward")
reg_summary <- summary(regfit_bwd)
which.min(reg_summary$bic)
reg_summary
which.min(reg_summary$bic)
str(college)
?createDataPartition
summary(reg_model)
which.min(reg_summary$bic)
reg_summary
reg_summary <- summary(regfit_bwd)
reg_summary
?plot
print(out)
box_plot <- function(acu) {
min <- min(acu, na.rm = TRUE)
q1st <- quantile(acu, probs = .25)
median <- quantile(acu, probs = .5)
q3st <- quantile(acu, probs = .75)
max <- max(acu, na.rm = TRUE)
out <- c(min, q1st, median, q3st, max)
boxplot(acu, horizontal = TRUE,
xlab = "number",
ylab = "acupuncture pk",
main = "box plot of acupuncture")
abline(v = min, lty = 2)
abline(v = q1st, lty = 2)
abline(v = median, lty = 2)
abline(v = q3st, lty = 2)
abline(v = max, lty = 2)
text(t(out))
print(out)
}
## draw boxplot for pk1
box_plot(acupuncture$pk1)
library(ggplot2)
load("acupuncture.Rdata")
## minimum
min(acupuncture$pk1, na.rm = TRUE)
min(acupuncture$pk5, na.rm = TRUE)
## 1st quantile
quantile(acupuncture$pk1, probs = 0.25)
quantile(acupuncture$pk5, probs = 0.25)
## median
median(acupuncture$pk1, na.rm = TRUE)
median(acupuncture$pk5, na.rm = TRUE)
## 3st quantile
quantile(acupuncture$pk1, probs = 0.75)
quantile(acupuncture$pk5, probs = 0.75)
## maximum
max(acupuncture$pk1, na.rm = TRUE)
max(acupuncture$pk5, na.rm = TRUE)
## @param acu acupuncture
## @return out min, q1st, median, q3st, max
call_out <- function(acu) {
min <- min(acu, na.rm = TRUE)
q1st <- quantile(acu, probs = .25)
median <- quantile(acu, probs = .5)
q3st <- quantile(acu, probs = .75)
max <- max(acu, na.rm = TRUE)
out <- c(min, q1st, median, q3st, max)
out
}
call_out(acupuncture$age)
call_out(acupuncture$sex)
call_out(acupuncture$chronicity)
call_out(acupuncture$pk1)
call_out(acupuncture$pk5)
# by(data = acupuncture$age,
#    INDICES = acupuncture$group,
#    FUN = call_out,
#    na.rm = TRUE)
box_plot <- function(acu) {
min <- min(acu, na.rm = TRUE)
q1st <- quantile(acu, probs = .25)
median <- quantile(acu, probs = .5)
q3st <- quantile(acu, probs = .75)
max <- max(acu, na.rm = TRUE)
out <- c(min, q1st, median, q3st, max)
boxplot(acu, horizontal = TRUE,
xlab = "number",
ylab = "acupuncture pk",
main = "box plot of acupuncture")
abline(v = min, lty = 2)
abline(v = q1st, lty = 2)
abline(v = median, lty = 2)
abline(v = q3st, lty = 2)
abline(v = max, lty = 2)
text(t(out))
print(out)
}
## draw boxplot for pk1
box_plot(acupuncture$pk1)
## draw boxplot for pk5
box_plot(acupuncture$pk5)
ridge_fit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
preProcess = c("scale", "center"))
ridge_fit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
preProcess = c("scale", "center"))
ridge_fit
ridge_grid <- data.frame(lambda = seq(0, 0.2, length = 15))
ridgefit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridgefit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridge_fit
library(lars)
ridgefit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridge_fit
1
ridge_fit <- train(Grad.Rate ~ ., data = train1, method = "ridge",
preProcess = c("scale", "center"))
ridge_fit
ridge_grid <- data.frame(lambda = seq(0.001, 10000, length = 50))
ridgefit2 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridgefit2 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridgefit2
library(ISLR2)
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
library(gam)
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
loess_model
loess_model2 <- train(Grad.Rate ~ ., span = 0.3 data = train1, method = "gamLoess")
loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
loess_model2
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
loess_model2
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
loess_model2 <- train(Grad.Rate ~ ., span = 1.5, data = train1, method = "gamLoess")
loess_model2
loess_model2 <- 0
loess_model2 <- train(Grad.Rate ~ ., span = 1.5, data = train1, method = "gamLoess")
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
loess_model
prediction <- predict(loess_model, test1$Grad.Rate)
library(leaps)
library(caret)
library(tidyverse)
library(lars)
library(ISLR2)
library(gam)
college <- read.csv("College.csv")
missing <- which(is.na(college))
missingCol <- floor(missing / nrow(college)) + 1
missingCol
college$PhD[is.na(college$PhD)] <- ceiling(mean(college$PhD, na.rm = TRUE))
college$Grad.Rate[is.na(college$Grad.Rate)] <- ceiling(
mean(college$Grad.Rate, na.rm = TRUE))
college <- college[, -1]
colnames(college)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
# testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
# postResample(pred = testprediction1, obs = train1$Grad.Rate)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I got Outstate and perc.alumni, intercept are with ***, which is in the interval of p <= 0.001
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = test1$Grad.Rate)
regfit_bwd <- regsubsets(Grad.Rate ~ ., data = train1, nvmax = 18, method = "backward")
reg_summary <- summary(regfit_bwd)
reg_summary
which.min(reg_summary$bic)
## in the last problem, I got my statistically significant variables are Outstate and perc.alumni. And for this, I got 7 bic, which are Apps, top25Perc, P.Undergrad, Outstate, Room.Board, perc.alumni, Expand. It got outstete and perc.alumni as similar.
ridge_fit1 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
preProcess = c("scale", "center"))
ridge_fit1
ridge_grid <- data.frame(lambda = seq(0.001, 10000, length = 50))
ridgefit2 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridgefit2
## lambda was chosen to 0.001
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
loess_model
prediction <- predict(loess_model, test1$Grad.Rate)
prediction <- predict(loess_model, test1$)
prediction <- predict(loess_model, test1)
prediction
RMSE(prediction$y, test1)
RMSE(prediction, test1)
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
summary(loess_model)
prediction <- predict(loess_model, test1)
RMSE(prediction, test1)
prediction <- predict(loess_model, test1$Grad.Rate)
RMSE(prediction, test1)
prediction <- predict(loess_model, test1$Grad.Rate)
View(test1)
View(train1)
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
summary(loess_model)
# loess_model
prediction <- predict(loess_model, test1$Grad.Rate)
# loess_model
prediction <- predict(loess_model, newdata =  test1$Grad.Rate)
# loess_model
prediction <- predict(loess_model, newdata =  test1$Grad.Rate)
# loess_model
prediction <- predict(loess_model, newdata =  test1)
RMSE(prediction, test1)
RMSE(prediction, test1$Grad.Rate)
gam_spline_model <- train(Grad.Rate ~ ., data = train1, method = "gamSpline")
prediction <- predict(gam_spline_model, newdata = test1)
RMSE(prediction, test1$Grad.Rate)
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
summary(loess_model)
loess_model
install.packages("leaps")
install.packages("caret")
install.packages("lars")
install.packages("ISLR2")
install.packages("gam")
library(leaps)
library(caret)
library(tidyverse)
library(lars)
library(ISLR2)
library(gam)
college <- read.csv("College.csv")
missing <- which(is.na(college))
missingCol <- floor(missing / nrow(college)) + 1
missingCol
college$PhD[is.na(college$PhD)] <- ceiling(mean(college$PhD, na.rm = TRUE))
college$Grad.Rate[is.na(college$Grad.Rate)] <- ceiling(
mean(college$Grad.Rate, na.rm = TRUE))
college <- college[, -1]
colnames(college)
intrain <- createDataPartition(college$Top10perc, p = 0.8, list = FALSE)
train1 <- college[intrain, ]
test1 <- college[-intrain, ]
reg_model <- train(Grad.Rate ~ . , data = train1, method ="lm")
summary(reg_model)
# testprediction1 <- predict(reg_model, newdata = train1, method = "lm")
# postResample(pred = testprediction1, obs = train1$Grad.Rate)
## I used top10perc as a dataset to split, and got a R-squared = 0.4515 and RSE = 13.01 on 605 degrees of freedom. I got Outstate and perc.alumni, intercept are with ***, which is in the interval of p <= 0.001
testpredictions <- predict(reg_model, newdata = test1, method = "lm")
postResample(pred = testpredictions, obs = test1$Grad.Rate)
regfit_bwd <- regsubsets(Grad.Rate ~ ., data = train1, nvmax = 18, method = "backward")
reg_summary <- summary(regfit_bwd)
reg_summary
which.min(reg_summary$bic)
## in the last problem, I got my statistically significant variables are Outstate and perc.alumni. And for this, I got 7 bic, which are Apps, top25Perc, P.Undergrad, Outstate, Room.Board, perc.alumni, Expand. It got outstete and perc.alumni as similar.
ridge_fit1 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
preProcess = c("scale", "center"))
ridge_fit1
ridge_grid <- data.frame(lambda = seq(0.001, 10000, length = 50))
ridgefit2 <- train(Grad.Rate ~ ., data = train1, method = "ridge",
tuneGrid = ridge_grid, preProcess = c("scale", "center"))
ridgefit2
## lambda was chosen to 0.001
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
# summary(loess_model)
loess_model
prediction <- predict(loess_model, newdata =  test1)
RMSE(prediction, test1$Grad.Rate)
gam_spline_model <- train(Grad.Rate ~ ., data = train1, method = "gamSpline")
prediction <- predict(gam_spline_model, newdata = test1)
RMSE(prediction, test1$Grad.Rate)
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
summary(loess_model)
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
# summary(loess_model)
loess_model
## the span chosen by caret is 0.5.
prediction <- predict(loess_model, newdata =  test1)
prediction
prediction
RMSE(prediction, test1$Grad.Rate)
prediction
loess_model <- train(Grad.Rate ~ ., data = train1, method = "gamLoess")
# loess_model2 <- train(Grad.Rate ~ ., span = 0.3, data = train1, method = "gamLoess")
# summary(loess_model)
loess_model
## the span chosen by caret is 0.5.
prediction <- predict(loess_model, newdata =  test1)
prediction
RMSE(prediction, test1$Grad.Rate)
prediction
summary(prediction)
RMSE(prediction, test1$Grad.Rate)
library(leaps)
library(caret)
library(tidyverse)
library(lars)
library(ISLR2)
library(gam)
college <- read.csv("College.csv")
college <- read.csv("College.csv")
library(leaps)
library(caret)
library(tidyverse)
library(lars)
library(ISLR2)
library(gam)
quarto install tool tinytex
